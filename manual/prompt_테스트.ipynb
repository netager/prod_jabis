{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from langchain_core.messages.chat import ChatMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain_core.callbacks.manager import CallbackManager\n",
    "\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import load_prompt\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain import hub\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_teddynote import logging\n",
    "import time\n",
    "import base64\n",
    "# from streamlit_pdf_viewer import pdf_viewer\n",
    "from urllib.parse import urlencode\n",
    "\n",
    "from operator import itemgetter\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough, Runnable\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# _type: \"prompt\"\n",
    "# template: |\n",
    "#   You are an assistant for question-answering tasks.\n",
    "#   Use the following pieces of retrieved `information` to answer the question with emoji. \n",
    "#   If you don't know the answer, just say that you don't know. \n",
    "  \n",
    "\n",
    "#   <information>\n",
    "#   {context}\n",
    "#   </information>\n",
    "\n",
    "#   #Question:\n",
    "#   {question} \n",
    "\n",
    "#   #Answer:\n",
    "# input_variables: [\"question\", \"context\"]\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True, memory_key=\"chat_history\")\n",
    "\n",
    "model_name_path = '../../HUGGING_FACE_MODEL/BAAI_bge-m3'\n",
    "\n",
    "def content_for_documents(document_list):\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in document_list])\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=model_name_path, \n",
    "        model_kwargs={\"device\": \"mps\"},  # cpu : 'cpu', macOS: 'mps', CUDA: 'cuda'\n",
    "        # 모델이 CPU에서 실행되도록 설정. GPU를 사용할 수 있는 환경이라면 'cuda'로 설정할 수도 있음\n",
    "        encode_kwargs={\n",
    "            \"normalize_embeddings\": True\n",
    "        },  # 임베딩 정규화. 모든 벡터가 같은 범위의 값을 갖도록 함. 유사도 계산 시 일관성을 높여줌\n",
    "        # cache_folder='../embedding/model',\n",
    "    )\n",
    "\n",
    "# Chroma db Loading\n",
    "chroma_db = Chroma(persist_directory=\"../../Chroma_DB/chroma_bank_law_db\",\n",
    "                embedding_function=embeddings, \n",
    "                collection_name=\"bank_law_case\")\n",
    "\n",
    "# Retriever 정의\n",
    "retriever = chroma_db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 2})\n",
    "\n",
    "prompt_type = \"\"\n",
    "# prompt_type = \"은행\"\n",
    "\n",
    "if prompt_type == \"은행\":\n",
    "    # prompt = load_prompt(\"../prompts/rag.yaml\")\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"A chat between a curious user and artificial intelligence assistant. The assistant gives simple answer to the user's questions.\"\n",
    "                # \"A chat between a curious user and artificial intelligence assistant. The assistant gives helpfule, detailed, and polite answer to the user's questions.\"                    \n",
    "                # \"당신은 친절한 20년차 은행원이면서 IT 전문가인 JABIS 입니다. 다음의 질문에 답변해 주세요.\",\n",
    "            ),\n",
    "            MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "            (\n",
    "                \"user\", \n",
    "                \"\"\"Human: <information>{context}</information>\\n\\n\n",
    "                   #Question: {question}\\nAssistant: \"\"\"\n",
    "            ),\n",
    "\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Embedding Call\n",
    "else:\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"A chat between a curious user and artificial intelligence assistant. The assistant gives simple answer to the user's questions.\"\n",
    "                # \"A chat between a curious user and artificial intelligence assistant. The assistant gives helpfule, detailed, and polite answer to the user's questions.\"                    \n",
    "                # \"당신은 친절한 20년차 은행원이면서 IT 전문가인 JABIS 입니다. 다음의 질문에 답변해 주세요.\",\n",
    "            ),\n",
    "            MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "            (\"user\", \"Human: {question}\\nAssistant: \"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "runnable = RunnablePassthrough.assign(\n",
    "    chat_history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"chat_history\")  # memory_key 와 동일하게 입력합니다.\n",
    ")\n",
    "\n",
    "if prompt_type == \"은행\":\n",
    "    chain = (\n",
    "        {\"context\": retriever | content_for_documents, \"question\": RunnablePassthrough(), \"chat_history\":runnable}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | output_parser\n",
    "    )\n",
    "else:\n",
    "    # from langchain_core.runnables import RunnableLambda\n",
    "    # from operator import itemgetter\n",
    "    chain = (\n",
    "        runnable\n",
    "        | prompt\n",
    "        | llm\n",
    "        | output_parser\n",
    "    )\n",
    "\n",
    "# user_input = \"대한민국의 수도는?\"\n",
    "# ai_answer = chain.invoke({\"question\": input})\n",
    "\n",
    "# memory.save_context({\"human\": user_input}, {\"assistant\": ai_answer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'hi!', 'chat_history': []}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runnable.invoke({\"input\": \"hi!\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {\"context\": retriever | content_for_documents, \"question\": RunnablePassthrough(), \"chat_history\":runnable}\n",
    "    | prompt | llm | output_parser)\n",
    "\n",
    "# chain = (runnable | prompt | llm | output_parser)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"서울의 수도가 어디야?\"\n",
    "user_input = \"내 이름은 이현우야 만나서 반가워\"\n",
    "ai_answer = chain.invoke({\"question\": user_input})\n",
    "\n",
    "memory.save_context({\"human\": user_input}, {\"assistant\": ai_answer})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'당신의 이름은 이현우입니다. 만나서 반가워요, 이현우님! 궁금한 점이나 도움이 필요하시면 언제든 말씀해주세요. 최선을 다해 도와드리겠습니다.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_input = \"내 이름이 뭐라고?\"\n",
    "chain.invoke({\"question\": user_input})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableAssign(mapper={\n",
       "  chat_history: RunnableLambda(load_memory_variables)\n",
       "                | RunnableLambda(itemgetter('chat_history'))\n",
       "})\n",
       "| ChatPromptTemplate(input_variables=['chat_history', 'question'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template=\"A chat between a curious user and artificial intelligence assistant. The assistant gives simple answer to the user's questions.\")), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], template='Human: {question}\\nAssistant: '))])\n",
       "| ChatOllama(model='EEVE-Korean-10.8B:latest')\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  context: VectorStoreRetriever(tags=['Chroma', 'HuggingFaceEmbeddings'], vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x17eaf2190>, search_kwargs={'k': 2})\n",
       "           | RunnableLambda(content_for_documents),\n",
       "  question: RunnablePassthrough(),\n",
       "  chat_history: RunnableAssign(mapper={\n",
       "                  chat_history: RunnableLambda(load_memory_variables)\n",
       "                                | RunnableLambda(itemgetter('chat_history'))\n",
       "                })\n",
       "}\n",
       "| ChatPromptTemplate(input_variables=['chat_history', 'question'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template=\"A chat between a curious user and artificial intelligence assistant. The assistant gives simple answer to the user's questions.\")), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], template='Human: {question}\\nAssistant: '))])\n",
       "| ChatOllama(model='EEVE-Korean-10.8B:latest')\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"A chat between a curious user and artificial intelligence assistant. The assistant gives simple answer to the user's questions.\"\n",
    "            # \"A chat between a curious user and artificial intelligence assistant. The assistant gives helpfule, detailed, and polite answer to the user's questions.\"                    \n",
    "            # \"당신은 친절한 20년차 은행원이면서 IT 전문가인 JABIS 입니다. 다음의 질문에 답변해 주세요.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\n",
    "            \"user\", \n",
    "            \"\"\"Human: <information>{context}</information>\\n\\n\n",
    "                #Question: {question}\\nAssistant: \"\"\"\n",
    "        ),\n",
    "\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever | content_for_documents, \"question\": RunnablePassthrough()}\n",
    "    | prompt | llm | output_parser)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import (\n",
    "    RunnableLambda,\n",
    "    RunnableParallel,\n",
    "    RunnablePassthrough,\n",
    ")    \n",
    "runnable = RunnableParallel(\n",
    "    context=retriever | content_for_documents,\n",
    "    question=RunnablePassthrough(),\n",
    "    chat_history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"chat_history\")  # memory_key 와 동일하게 입력합니다.\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  context: VectorStoreRetriever(tags=['Chroma', 'HuggingFaceEmbeddings'], vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x17eaf2190>, search_kwargs={'k': 2})\n",
       "           | RunnableLambda(content_for_documents),\n",
       "  question: RunnableLambda(RunnablePassthrough),\n",
       "  chat_history: RunnableLambda(load_memory_variables)\n",
       "                | RunnableLambda(itemgetter('chat_history'))\n",
       "}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runnable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runnable.invoke({\"question\": \"서울의 수도가 어디니?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  context: VectorStoreRetriever(tags=['Chroma', 'HuggingFaceEmbeddings'], vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x17eaf2190>, search_kwargs={'k': 2})\n",
       "           | RunnableLambda(content_for_documents),\n",
       "  question: RunnablePassthrough()\n",
       "}\n",
       "| ChatPromptTemplate(input_variables=['chat_history', 'context', 'question'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template=\"A chat between a curious user and artificial intelligence assistant. The assistant gives simple answer to the user's questions.\")), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template='Human: <information>{context}</information>\\n\\n\\n                #Question: {question}\\nAssistant: '))])\n",
       "| ChatOllama(model='EEVE-Korean-10.8B:latest')\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(return_messages=True, memory_key=\"chat_history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Expected a Runnable, callable or dict.Instead got an unsupported type: <class 'str'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Retriever 정의\u001b[39;00m\n\u001b[1;32m     22\u001b[0m retriever \u001b[38;5;241m=\u001b[39m chroma_db\u001b[38;5;241m.\u001b[39mas_retriever(search_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimilarity\u001b[39m\u001b[38;5;124m\"\u001b[39m, search_kwargs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mk\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m2\u001b[39m})\n\u001b[0;32m---> 24\u001b[0m runnable \u001b[38;5;241m=\u001b[39m \u001b[43mRunnablePassthrough\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massign\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchat_history\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mRunnableLambda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmemory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_memory_variables\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m|\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mitemgetter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchat_history\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mabc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mabc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# memory_key 와 동일하게 입력합니다.\u001b[39;49;00m\n\u001b[1;32m     27\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# runnable = RunnableParallel(\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m#     chat_history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"chat_history\")  # memory_key 와 동일하게 입력합니다.\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[1;32m     33\u001b[0m prompt \u001b[38;5;241m=\u001b[39m ChatPromptTemplate\u001b[38;5;241m.\u001b[39mfrom_messages(\n\u001b[1;32m     34\u001b[0m     [\n\u001b[1;32m     35\u001b[0m         (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     43\u001b[0m     ]\n\u001b[1;32m     44\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/prod_llm/lib/python3.11/site-packages/langchain_core/runnables/passthrough.py:208\u001b[0m, in \u001b[0;36mRunnablePassthrough.assign\u001b[0;34m(cls, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21massign\u001b[39m(\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    197\u001b[0m     ],\n\u001b[1;32m    198\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunnableAssign\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    199\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Merge the Dict input with the output produced by the mapping argument.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \n\u001b[1;32m    201\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;124;03m        mapping argument.\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m RunnableAssign(\u001b[43mRunnableParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/prod_llm/lib/python3.11/site-packages/langchain_core/runnables/base.py:3015\u001b[0m, in \u001b[0;36mRunnableParallel.__init__\u001b[0;34m(self, steps__, **kwargs)\u001b[0m\n\u001b[1;32m   3012\u001b[0m merged \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msteps__} \u001b[38;5;28;01mif\u001b[39;00m steps__ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m   3013\u001b[0m merged\u001b[38;5;241m.\u001b[39mupdate(kwargs)\n\u001b[1;32m   3014\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m-> 3015\u001b[0m     steps__\u001b[38;5;241m=\u001b[39m\u001b[43m{\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoerce_to_runnable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmerged\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m   3016\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/prod_llm/lib/python3.11/site-packages/langchain_core/runnables/base.py:3015\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   3012\u001b[0m merged \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msteps__} \u001b[38;5;28;01mif\u001b[39;00m steps__ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m   3013\u001b[0m merged\u001b[38;5;241m.\u001b[39mupdate(kwargs)\n\u001b[1;32m   3014\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m-> 3015\u001b[0m     steps__\u001b[38;5;241m=\u001b[39m{key: \u001b[43mcoerce_to_runnable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m key, r \u001b[38;5;129;01min\u001b[39;00m merged\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m   3016\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/prod_llm/lib/python3.11/site-packages/langchain_core/runnables/base.py:5044\u001b[0m, in \u001b[0;36mcoerce_to_runnable\u001b[0;34m(thing)\u001b[0m\n\u001b[1;32m   5042\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(Runnable[Input, Output], RunnableParallel(thing))\n\u001b[1;32m   5043\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 5044\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   5045\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected a Runnable, callable or dict.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   5046\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInstead got an unsupported type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(thing)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   5047\u001b[0m     )\n",
      "\u001b[0;31mTypeError\u001b[0m: Expected a Runnable, callable or dict.Instead got an unsupported type: <class 'str'>"
     ]
    }
   ],
   "source": [
    "model_name_path = '../../HUGGING_FACE_MODEL/BAAI_bge-m3'\n",
    "\n",
    "def content_for_documents(document_list):\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in document_list])\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=model_name_path, \n",
    "        model_kwargs={\"device\": \"mps\"},  # cpu : 'cpu', macOS: 'mps', CUDA: 'cuda'\n",
    "        # 모델이 CPU에서 실행되도록 설정. GPU를 사용할 수 있는 환경이라면 'cuda'로 설정할 수도 있음\n",
    "        encode_kwargs={\n",
    "            \"normalize_embeddings\": True\n",
    "        },  # 임베딩 정규화. 모든 벡터가 같은 범위의 값을 갖도록 함. 유사도 계산 시 일관성을 높여줌\n",
    "        # cache_folder='../embedding/model',\n",
    "    )\n",
    "\n",
    "# Chroma db Loading\n",
    "chroma_db = Chroma(persist_directory=\"../../Chroma_DB/chroma_bank_law_db\",\n",
    "                embedding_function=embeddings, \n",
    "                collection_name=\"bank_law_case\")\n",
    "\n",
    "# Retriever 정의\n",
    "retriever = chroma_db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 2})\n",
    "\n",
    "runnable = RunnablePassthrough.assign(\n",
    "    chat_history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"chat_history\")  # memory_key 와 동일하게 입력합니다.\n",
    ")\n",
    "\n",
    "# runnable = RunnableParallel(\n",
    "#     chat_history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"chat_history\")  # memory_key 와 동일하게 입력합니다.\n",
    "# )\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"A chat between a curious user and artificial intelligence assistant. The assistant gives simple answer to the user's questions.\"\n",
    "            # \"A chat between a curious user and artificial intelligence assistant. The assistant gives helpfule, detailed, and polite answer to the user's questions.\"                    \n",
    "            # \"당신은 친절한 20년차 은행원이면서 IT 전문가인 JABIS 입니다. 다음의 질문에 답변해 주세요.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"user\", \"Human: {question}\\nAssistant: \"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = (\n",
    "    runnable\n",
    "    | prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableAssign(mapper={\n",
       "  chat_history: RunnableLambda(load_memory_variables)\n",
       "                | RunnableLambda(itemgetter('chat_history'))\n",
       "})\n",
       "| ChatPromptTemplate(input_variables=['chat_history', 'question'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template=\"A chat between a curious user and artificial intelligence assistant. The assistant gives simple answer to the user's questions.\")), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], template='Human: {question}\\nAssistant: '))])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain\n",
    "# chain.invoke({\"question\":\"서울의 수도가 어디니?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  chat_history: RunnableLambda(load_memory_variables)\n",
       "                | RunnableLambda(itemgetter('chat_history'))\n",
       "}\n",
       "| ChatPromptTemplate(input_variables=['chat_history', 'question'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template=\"A chat between a curious user and artificial intelligence assistant. The assistant gives simple answer to the user's questions.\")), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], template='Human: {question}\\nAssistant: '))])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain\n",
    "# chain.invoke({\"question\":\"서울의 수도가 어디니?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"대한민국의 수도는 어디야?\"\n",
    "ai_answer = \"대한민국의 수도는 전주야\"\n",
    "\n",
    "memory.save_context({\"human\": user_input}, {\"assistant\": ai_answer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableAssign(mapper={\n",
       "  chat_history: RunnableLambda(load_memory_variables)\n",
       "                | RunnableLambda(itemgetter('chat_history'))\n",
       "})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runnable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runnable = RunnableParallel(\n",
    "#     context=retriever | content_for_documents,\n",
    "#     question=RunnablePassthrough(),\n",
    "#     chat_history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"chat_history\")  # memory_key 와 동일하게 입력합니다.\n",
    "# )\n",
    "\n",
    "runnable = RunnableParallel(\n",
    "    chat_history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"chat_history\")  # memory_key 와 동일하게 입력합니다.\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  chat_history: RunnableLambda(load_memory_variables)\n",
       "                | RunnableLambda(itemgetter('chat_history'))\n",
       "}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runnable"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prod_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
