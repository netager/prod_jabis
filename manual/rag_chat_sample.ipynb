{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith 추적을 시작합니다.\n",
      "[프로젝트명]\n",
      "PROD_RAG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/prod_llm/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain_teddynote import logging\n",
    "from operator import itemgetter\n",
    "\n",
    "\n",
    "# .env 환경 변수 로딩\n",
    "# ----------------\n",
    "load_dotenv()\n",
    "\n",
    "# LangSmith를 이용하여 LLM 추적\n",
    "# -------------------------\n",
    "logging.langsmith(\"PROD_RAG\", set_enable=True)  # enable\n",
    "\n",
    "\n",
    "model_name_path = \"../../HUGGING_FACE_MODEL/BAAI_bge-m3\"\n",
    "def embeddings_call():\n",
    "    return HuggingFaceEmbeddings(\n",
    "        model_name=model_name_path,\n",
    "        model_kwargs={\"device\": \"mps\"},  # cpu : 'cpu', macOS: 'mps', CUDA: 'cuda'\n",
    "        # 모델이 CPU에서 실행되도록 설정. GPU를 사용할 수 있는 환경이라면 'cuda'로 설정할 수도 있음\n",
    "        encode_kwargs={\n",
    "            \"normalize_embeddings\": True\n",
    "        },  # 임베딩 정규화. 모든 벡터가 같은 범위의 값을 갖도록 함. 유사도 계산 시 일관성을 높여줌\n",
    "        # cache_folder='../embedding/model',\n",
    "    )\n",
    "\n",
    "# vectordb = FAISS.load_local(persist_directory, embedding, index_name, allow_dangerous_deserialization=True)\n",
    "embeddings = embeddings_call()\n",
    "vectordb = Chroma(\n",
    "            persist_directory=\"../../Chroma_DB/chroma_bank_law_db\",\n",
    "            embedding_function=embeddings,\n",
    "            collection_name=\"bank_law_case\",\n",
    "        )\n",
    "\n",
    "retriever = vectordb.as_retriever(\n",
    "    search_type=\"similarity\", search_kwargs={\"k\": 1}\n",
    ")\n",
    "\n",
    "\n",
    "llm = ChatOllama(model=\"EEVE-Korean-10.8B:latest\")\n",
    "\n",
    "contextualize_q_system_prompt = \"\"\"Given a chat history and the latest user question \\\n",
    "which might reference context in the chat history, formulate a standalone question \\\n",
    "which can be understood without the chat history. Do NOT answer the question, \\\n",
    "just reformulate it if needed and otherwise return it as is.\"\"\"\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")\n",
    "\n",
    "\n",
    "qa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\\n",
    "Use the following pieces of retrieved context to answer the question. \\\n",
    "If you don't know the answer, just say that you don't know. \\\n",
    "Use three sentences maximum and keep the answer concise.\\\n",
    "\\n\\n\n",
    "{context}\"\"\"\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# question_answer_chain = create_stuff_documents_chain(llm, qa_prompt, output_parser=parser)\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "\n",
    "store = {}\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'전산운영위원회의 상임위원으로는 다음과 같은 분들이 있습니다: 전산담당임원, 종합기획부장, 인사지원부장, 마케팅기획부장, 준법감시부장, IT기획부장, IT개발부장, 정보보호부장, 디지털플랫폼부장. 위원회의 위원장은 전산담당본부장이 맡으며, 유고 시에는 IT기획부장이 직무대행을 합니다. 또한 심의사항 관련 부서의 담당 본부장과 부·실장으로 구성된 위촉위원도 있습니다.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_rag_chain.invoke(\n",
    "    # {\"input\": \"What is Task Decomposition?\"},\n",
    "    {\"input\": \"전산운영위원회 위원을 알려줘?\"},\n",
    "    config={\n",
    "        \"configurable\": {\"session_id\": \"abc123\"}\n",
    "    },\n",
    ")[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    query = input(\"전산운영위원회 위원을 알려줘\")\n",
    "\n",
    "    for chunk in conversational_rag_chain.stream(\n",
    "        {\"input\": query,},\n",
    "        config={\n",
    "            \"configurable\": {\n",
    "                \"session_id\": \"demo_1\"\n",
    "            }\n",
    "        }\n",
    "    ):\n",
    "        if answer_chunk := chunk.get(\"answer\"):\n",
    "            print(f\"{answer_chunk}\", end=\"\", flush=True)\n",
    "    \n",
    "    # print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith 추적을 시작합니다.\n",
      "[프로젝트명]\n",
      "PROD_RAG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/prod_llm/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "# from langchain.chains import create_retrieval_chain\n",
    "# from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "# from langchain_core.chat_history import BaseChatMessageHistory\n",
    "# from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "# from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain_teddynote import logging\n",
    "from operator import itemgetter\n",
    "\n",
    "\n",
    "# .env 환경 변수 로딩\n",
    "# ----------------\n",
    "load_dotenv()\n",
    "\n",
    "# LangSmith를 이용하여 LLM 추적\n",
    "# -------------------------\n",
    "logging.langsmith(\"PROD_RAG\", set_enable=True)  # enable\n",
    "\n",
    "\n",
    "model_name_path = \"../../HUGGING_FACE_MODEL/BAAI_bge-m3\"\n",
    "def embeddings_call():\n",
    "    return HuggingFaceEmbeddings(\n",
    "        model_name=model_name_path,\n",
    "        model_kwargs={\"device\": \"mps\"},  # cpu : 'cpu', macOS: 'mps', CUDA: 'cuda'\n",
    "        # 모델이 CPU에서 실행되도록 설정. GPU를 사용할 수 있는 환경이라면 'cuda'로 설정할 수도 있음\n",
    "        encode_kwargs={\n",
    "            \"normalize_embeddings\": True\n",
    "        },  # 임베딩 정규화. 모든 벡터가 같은 범위의 값을 갖도록 함. 유사도 계산 시 일관성을 높여줌\n",
    "        # cache_folder='../embedding/model',\n",
    "    )\n",
    "\n",
    "# vectordb = FAISS.load_local(persist_directory, embedding, index_name, allow_dangerous_deserialization=True)\n",
    "embeddings = embeddings_call()\n",
    "vectordb = Chroma(\n",
    "            persist_directory=\"../../Chroma_DB/chroma_bank_law_db\",\n",
    "            embedding_function=embeddings,\n",
    "            collection_name=\"bank_law_case\",\n",
    "        )\n",
    "\n",
    "retriever = vectordb.as_retriever(\n",
    "    search_type=\"similarity\", search_kwargs={\"k\": 1}\n",
    ")\n",
    "\n",
    "\n",
    "llm = ChatOllama(model=\"EEVE-Korean-10.8B:latest\")\n",
    "\n",
    "'''\n",
    "contextualize_q_system_prompt = \"\"\"Given a chat history and the latest user question \\\n",
    "which might reference context in the chat history, formulate a standalone question \\\n",
    "which can be understood without the chat history. Do NOT answer the question, \\\n",
    "just reformulate it if needed and otherwise return it as is.\"\"\"\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")\n",
    "'''\n",
    "\n",
    "qa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\\n",
    "Use the following pieces of retrieved context to answer the question. \\\n",
    "If you don't know the answer, just say that you don't know. \\\n",
    "Use three sentences maximum and keep the answer concise.\\\n",
    "\\n\\n\n",
    "{context}\"\"\"\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": itemgetter(\"input\") | retriever,\n",
    "        \"input\": itemgetter(\"input\"),\n",
    "    }\n",
    "    | qa_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "# # question_answer_chain = create_stuff_documents_chain(llm, qa_prompt, output_parser=parser)\n",
    "# question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "# rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "\n",
    "\n",
    "# store = {}\n",
    "# def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "#     if session_id not in store:\n",
    "#         store[session_id] = ChatMessageHistory()\n",
    "#     return store[session_id]\n",
    "\n",
    "# conversational_rag_chain = RunnableWithMessageHistory(\n",
    "#     rag_chain,\n",
    "#     get_session_history,\n",
    "#     input_messages_key=\"input\",\n",
    "#     history_messages_key=\"chat_history\",\n",
    "#     output_messages_key=\"answer\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'해당 문서에 따르면, 전북은행의 전산운영위원회 상임위원은 다음과 같습니다:\\n\\n1. 전산담당임원\\n2. 종합기획부장\\n3. 인사지원부장\\n4. 마케팅기획부장\\n5. 준법감시부장\\n6. IT기획부장\\n7. IT개발부장\\n8. 정보보호부장\\n9. 디지털플랫폼부장 (2022년 8월 8일 개정)\\n\\n위촉위원은 위원회의 심사에 관련된 부서의 담당 본부장과 부·실장으로 구성되며, 위원회 위원장이 선임합니다.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\n",
    "    {\"input\": \"전산운영위원회 위원을 알려줘?\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "해당 문서에 따르면, 전북은행의 전산운영위원회 상임위원은 다음과 같습니다:\n",
      "\n",
      "1. 전산담당임원\n",
      "2. 종합기획부장\n",
      "3. 인사지원부장\n",
      "4. 마케팅기획부장\n",
      "5. 준법감시부장\n",
      "6. IT기획부장\n",
      "7. IT개발부장\n",
      "8. 정보보호부장\n",
      "9. 디지털플랫폼부장 (2022년 8월 8일 개정)\n",
      "\n",
      "위촉위원은 위원회의 심사에 관련된 부서의 담당 본부장과 부·실장으로 구성되며, 위원회 위원장이 선임합니다."
     ]
    }
   ],
   "source": [
    "from langchain_teddynote.messages import stream_response\n",
    "\n",
    "answer = chain.stream(\n",
    "    {\"input\": \"전산운영위원회 위원을 알려줘?\"},\n",
    ")\n",
    "stream_response(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from langchain_teddynote.messages import stream_response\n",
    "stream_response(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'해당 문서에 따르면, 전북은행의 전산운영위원회 상임위원은 다음과 같습니다:\\n\\n1. 전산담당임원\\n2. 종합기획부장\\n3. 인사지원부장\\n4. 마케팅기획부장\\n5. 준법감시부장\\n6. IT기획부장\\n7. IT개발부장\\n8. 정보보호부장\\n9. 디지털플랫폼부장 (2022년 8월 8일 개정)\\n\\n위촉위원은 위원회의 심사에 관련된 부서의 담당 본부장과 부·실장으로 구성되며, 위원회 위원장이 선임합니다.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\n",
    "    {\"input\": \"전산운영위원회 위원을 알려줘?\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.stream(\n",
    "    {\"input\": \"전산운영위원회 위원을 알려줘?\"},\n",
    ")\n",
    "ai_answer = \"\"\n",
    "for token in response:\n",
    "    ai_answer += token\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prod_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
