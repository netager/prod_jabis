import streamlit as st
from langchain_core.messages.chat import ChatMessage
from langchain_core.prompts import (
    ChatPromptTemplate,
    MessagesPlaceholder,
    PromptTemplate,
)
from langchain_openai import ChatOpenAI
from langchain_community.chat_models import ChatOllama
from langchain_core.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from langchain_core.callbacks.manager import CallbackManager

from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma

from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_core.prompts import load_prompt

# from langchain.memory import ConversationBufferMemory
from langchain import hub

from dotenv import load_dotenv

from langchain_teddynote import logging
import time
from datetime import datetime
import base64
from urllib.parse import urlencode

from operator import itemgetter
from langchain.memory import ConversationBufferMemory
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables import RunnableLambda, RunnablePassthrough, Runnable
from langchain_core.chat_history import InMemoryChatMessageHistory
from langchain_community.chat_message_histories import ChatMessageHistory


# ì²˜ë¦¬ì‹œê°„ í™•ì¸
# ----------
def get_cur_time():
    return datetime.now().strftime("%Y-%m-%d %H:%M:%S")

LOG_STATUS = "CONSOLE"
def log_writer(LOG_STATUS, message):
    if LOG_STATUS == "CONSOLE_MONITOR":
        st.write(f"[{get_cur_time()}] {message}")
        print(f"[{get_cur_time()}] {message}")
    elif LOG_STATUS == "CONSOLE":
        print(f"[{get_cur_time()}] {message}")
    elif LOG_STATUS == "MONITOR":
        st.write(f"[{get_cur_time()}] {message}")

log_writer(LOG_STATUS, "(jabis_chat.py) Jabis Program Start")

# .env í™˜ê²½ ë³€ìˆ˜ ë¡œë”©
# ----------------
load_dotenv()

# LangSmithë¥¼ ì´ìš©í•˜ì—¬ LLM ì¶”ì 
# -------------------------
# logging.langsmith("PROD_LLM", set_enable=True)  # enable


# session id ê°€ì ¸ì˜¤ê¸°
# -----------------
from streamlit.runtime.scriptrunner.script_run_context import get_script_run_ctx

ctx = get_script_run_ctx()
session_id = ctx.session_id


# ëŒ€í™” ë²„í¼ ë©”ëª¨ë¦¬ë¥¼ ìƒì„±í•˜ê³ , ë©”ì‹œì§€ ë°˜í™˜ ê¸°ëŠ¥ì„ í™œì„±í™”í•©ë‹ˆë‹¤.
# ----------------------------------------------
# if "chat_memory" not in st.session_state:
#     st.session_state["chat_memory"] = ConversationBufferMemory(return_messages=True, memory_key="chat_history")
# memory = st.session_state["chat_memory"]

if "chat_session" not in st.session_state:
    st.session_state["chat_session"] = {}
chat_store = st.session_state["chat_session"]


def get_session_history(session_ids):
    st.write(f"[ëŒ€í™” ì„¸ì…˜ID]: {session_ids}")
    if session_ids not in chat_store:  # ì„¸ì…˜ IDê°€ storeì— ì—†ëŠ” ê²½ìš°
        # chat_store[session_ids] = ChatMessageHistory()
        chat_store[session_ids] = InMemoryChatMessageHistory()
    # else:
    #     store[session_ids] = InMemoryChatMessageHistory(messages=messages[:-1])
    return chat_store[session_ids]  # í•´ë‹¹ ì„¸ì…˜ IDì— ëŒ€í•œ ì„¸ì…˜ ê¸°ë¡ ë°˜í™˜


# -----------------------------------------------------------------------------
# Embedding Model Caching
# -----------------------------------------------------------------------------
# Caching Embedding Model
# -----------------------
model_name_path = "../HUGGING_FACE_MODEL/BAAI_bge-m3"
@st.cache_resource()
def embeddings_call():
    return HuggingFaceEmbeddings(
        model_name=model_name_path,
        model_kwargs={"device": "mps"},  # cpu : 'cpu', macOS: 'mps', CUDA: 'cuda'
        # ëª¨ë¸ì´ CPUì—ì„œ ì‹¤í–‰ë˜ë„ë¡ ì„¤ì •. GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” í™˜ê²½ì´ë¼ë©´ 'cuda'ë¡œ ì„¤ì •í•  ìˆ˜ë„ ìˆìŒ
        encode_kwargs={
            "normalize_embeddings": True
        },  # ì„ë² ë”© ì •ê·œí™”. ëª¨ë“  ë²¡í„°ê°€ ê°™ì€ ë²”ìœ„ì˜ ê°’ì„ ê°–ë„ë¡ í•¨. ìœ ì‚¬ë„ ê³„ì‚° ì‹œ ì¼ê´€ì„±ì„ ë†’ì—¬ì¤Œ
        # cache_folder='../embedding/model',
    )


@st.cache_resource()
def llm_model_call():
    return ChatOllama(model="Linkbricks-Llama3.1-Korean-8B-Q8_0:latest")
    # return ChatOllama(model="EEVE-Korean-10.8B:latest")
    # return ChatOllama(model="Llama-3.1-Korean-8B-Instruct_q8_0")
    # llm = ChatOllama(model="qwen2-7b-instruct-q8:latest", temperature=0)
    # llm = ChatOpenAI(model_name="gpt-4o", temperature=0,)
    # llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0,)
    

# -----------------------------------------------------------------------------
# Class Define
# -----------------------------------------------------------------------------
class MyConversationChain(Runnable):

    def __init__(self, llm, prompt, memory, input_key="question"):
        self.prompt = prompt
        self.memory = memory
        self.input_key = input_key

        self.chain = (
            RunnablePassthrough.assign(
                chat_history=RunnableLambda(self.memory.load_memory_variables)
                | itemgetter(self.memory.memory_key)
            )
            | prompt
            | llm
            | StrOutputParser()
        )

    def invoke(self, query, configs=None, **kwargs):
        answer = self.chain.invoke({self.input_key: query})
        self.memory.save_context(inputs={"human": query}, outputs={"ai": answer})
        return answer


# -----------------------------------------------------------------------------
# í•¨ìˆ˜ Define
# -----------------------------------------------------------------------------
# pdf ì¶œë ¥ì„ ìœ„í•œ í•¨ìˆ˜
# ----------------
def displayPDF(file, page):
    # Opening file from file path
    with open(file, "rb") as f:
        base64_pdf = base64.b64encode(f.read()).decode("utf-8")

    # Embedding PDF in HTML
    # pdf_display = F'<iframe src="data:application/pdf;base64,{base64_pdf}" width="700" height="650" type="application/pdf"></iframe>'
    pdf_display = f'<iframe src="data:application/pdf;base64,{base64_pdf}#page={page}" width="100%" height="750" type="application/pdf"></iframe>'
    # pdf_display = F'<embed src="data:application/pdf;base64,{base64_pdf}" width="700" height="700" type="application/pdf">'
    # pdf_display = F'<iframe src="{file}#toolbar=0 page={page}"></iframe>'

    # Displaying File
    st.markdown(pdf_display, unsafe_allow_html=True)


# ì„¸ì…˜ì— ì €ì¥ëœ ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ì¶œë ¥í•˜ëŠ” í•¨ìˆ˜
# ---------------------------------
def print_saved_messages():
    for chat_message in st.session_state["messages"]:
        st.chat_message(chat_message.role).write(chat_message.content)


# ì„¸ì…˜ì— ëŒ€í™” ë‚´ìš© ì €ì¥í•˜ëŠ”ë° ChatMessage í˜•ì‹ìœ¼ë¡œ ì €ì¥
# -------------------------------------------
def add_message(role, message):
    st.session_state["messages"].append(ChatMessage(role=role, content=message))


# retriever(k=3) ê²°ê³¼ê°€ ì—¬ëŸ¬ê°œ ì¼ë•Œ ì²˜ë¦¬ ë°©ë²•
# Documentsì˜ ë©”íƒ€ëŠ” ì œì™¸í•˜ê³  page_contentë§Œ ì ìš©
# -----------------------------------------
def content_for_documents(document_list):
    return "\n\n".join([doc.page_content for doc in document_list])


# ì²´ì¸ ìƒì„± í•¨ìˆ˜
# -----------
def create_chain(prompt_type, user_input):
    # ë³€ìˆ˜ ì´ˆê¸°í™”
    search_results = None

    # RAG Prompt
    if prompt_type == "ì€í–‰ì—…ë¬´ ì§ˆì˜":
        prompt = load_prompt("./prompts/rag.yaml")
        # prompt = PromptTemplate.from_template(


        # prompt = PromptTemplate.from_template(
        #     """You are an assistant for question-answering tasks. 
        #     Use the following pieces of retrieved context to answer the question. 
        #     If you don't know the answer, just say that you don't know. 

        #     #Question: 
        #     {question} 

        #     #Previous Chat History:
        #     {chat_history}

        #     #Context: 
        #     {context} 

        #     #Answer:"""
        # )

        # prompt = ChatPromptTemplate.from_messages(
        #     [
        #         (
        #             "system",
        #             "A chat between a curious user and artificial intelligence assistant. The assistant gives simple answer to the user's questions."
        #             # "A chat between a curious user and artificial intelligence assistant. The assistant gives helpfule, detailed, and polite answer to the user's questions."
        #             # "ë‹¹ì‹ ì€ ì¹œì ˆí•œ 20ë…„ì°¨ ì€í–‰ì›ì´ë©´ì„œ IT ì „ë¬¸ê°€ì¸ JABIS ì…ë‹ˆë‹¤. ë‹¤ìŒì˜ ì§ˆë¬¸ì— ë‹µë³€í•´ ì£¼ì„¸ìš”.",
        #         ),
        #         MessagesPlaceholder(variable_name="chat_history"),
        #         (
        #             "user",
        #             """Human: <information>{context}</information>\n\n
        #             #Question: {question}\nAssistant: """
        #         ),

        #     ]
        # )

        # Embedding Call
        log_writer(LOG_STATUS, "(jabis_chat.py) embeddings_call() Start")
        embeddings = embeddings_call()

        # Chroma db Loading
        log_writer(LOG_STATUS, "(jabis_chat.py) Chroma() Loading Start")
        chroma_db = Chroma(
            persist_directory="../Chroma_DB/chroma_bank_law_db",
            embedding_function=embeddings,
            collection_name="bank_law_case",
        )


        # ëŒ€ìƒ ë¬¸ì„œë¥¼ ë³´ì—¬ì£¼ê¸° ìœ„í•´ vector store ê²€ìƒ‰
        log_writer(LOG_STATUS, "(jabis_chat.py) similarity_search_with_score() k=5")
        search_results = chroma_db.similarity_search_with_score(query=user_input, k=5)

        # Retriever ì •ì˜
        retriever = chroma_db.as_retriever(
            search_type="similarity", search_kwargs={"k": 1}
        )
        # retriever = chroma_db.as_retriever(search_type="mmr", search_kwargs={"k": 1})
        # retriever = chroma_db.as_retriever(search_type="similarity_score_threshold", search_kwargs={"score_threshold": 0.5, "k": 1})
        # search_result = retriever.get_relevant_documents(user_input, k=5)
        # search_results = retriever.invoke(user_input)
        # st.markdown(search_results)

    # í”„ë¡¬í”„íŠ¸(ê¸°ë³¸ëª¨ë“œ)
    else:
        search_results = ""
        prompt = ChatPromptTemplate.from_messages(
            [
                (
                    "system",
                    """You are Jabis of Jeonbuk Bank. A chat between a curious user and artificial intelligence assistant.
                     The assistant gives simple answer to the user's questions."""
                    # Answer in Englist and Korean.""",
                    # "A chat between a curious user and artificial intelligence assistant. The assistant gives helpfule, detailed, and polite answer to the user's questions."
                    # "ë‹¹ì‹ ì€ ì¹œì ˆí•œ 20ë…„ì°¨ ì€í–‰ì›ì´ë©´ì„œ IT ì „ë¬¸ê°€ì¸ JABIS ì…ë‹ˆë‹¤. ë‹¤ìŒì˜ ì§ˆë¬¸ì— ë‹µë³€í•´ ì£¼ì„¸ìš”.",
                ),
                MessagesPlaceholder(variable_name="chat_history"),
                ("user", "Human: {question}\nAssistant: "),
            ]
        )

    # LLM ëª¨ë¸ ì„ íƒ
    # -----------
    log_writer(LOG_STATUS, "(jabis_chat.py) llm_model_call() Start")
    llm = llm_model_call()

    # ì¶œë ¥ íŒŒì„œ
    output_parser = StrOutputParser()

    # ì²´ì¸ ìƒì„±
    log_writer(LOG_STATUS, "(jabis_chat.py) create chain Start")
    if prompt_type == "ì€í–‰ì—…ë¬´ ì§ˆì˜":
        # runnable = RunnablePassthrough.assign(
        #     chat_history=RunnableLambda(memory.load_memory_variables) | itemgetter("chat_history")  # memory_key ì™€ ë™ì¼í•˜ê²Œ ì…ë ¥í•©ë‹ˆë‹¤.
        # )
                # "chat_history": itemgetter("chat_history"),
        chain_with_history = (
            {
                "context":itemgetter("question") | retriever | content_for_documents,
                "question": itemgetter("question"),
            }
            | prompt
            | llm
            | output_parser
        )
        
    else:
        # from langchain_core.runnables import RunnableLambda
        # from operator import itemgetter

        # runnable = RunnablePassthrough.assign(
        #     chat_history=RunnableLambda(memory.load_memory_variables) | itemgetter("chat_history")  # memory_key ì™€ ë™ì¼í•˜ê²Œ ì…ë ¥í•©ë‹ˆë‹¤.
        # )

        chain = (
            # {
            #     "question": itemgetter("question"),
            #     "chat_history": itemgetter("chat_history"),
            # }
            # | prompt
            prompt
            | llm
            | output_parser
        )

        # for Class : TODO ~ You must implement stream method
        # ---------------------------------------------------
        # conversation_chain = MyConversationChain(llm, prompt, memory)

        chain_with_history = RunnableWithMessageHistory(
            chain,
            get_session_history,
            input_messages_key="question",
            history_messages_key="chat_history",
        )

    return chain_with_history, search_results
    # return conversation_chain, search_results


# ì²«ì¸ì‚¬ë§ ì„ íƒí•˜ê¸°
# -----------------
def greeting():
    import random

    initial_messages = [
        "ì•ˆë…•í•˜ì„¸ìš”. ë§Œë‚˜ëµ™ê²Œ ë˜ì–´ ë°˜ê°‘ìŠµë‹ˆë‹¤. ì €ëŠ” ì „ë¶ì€í–‰ì—ì„œ ê·¼ë¬´í•˜ëŠ” Chat ìƒë‹´ì› Jabisì…ë‹ˆë‹¤.\n\n ì™¼ìª½ ë©”ë‰´ì˜ [í™œìš© ì—…ë¬´ ì„ íƒ]ì„ í™•ì¸í•˜ê³  ê¶ê¸ˆí•œ ì‚¬í•­ì´ ìˆìœ¼ë©´ ì•„ë˜ì— ì…ë ¥í•´ ì£¼ì„¸ìš”.",
        "ì¢‹ì€ë‚  ì…ë‹ˆë‹¤. Jabis ì…ë‹ˆë‹¤. ë§Œë‚˜ëµ™ê²Œ ë˜ì–´ ë°˜ê°‘ìŠµë‹ˆë‹¤.\n\n ì™¼ìª½ ë©”ë‰´ì˜ [í™œìš© ì—…ë¬´ ì„ íƒ]ì„ í™•ì¸í•˜ê³  ê¶ê¸ˆí•œ ì‚¬í•­ì´ ìˆìœ¼ë©´ ì•„ë˜ì— ì…ë ¥í•´ ì£¼ì„¸ìš”.",
        "Jabis ì…ë‹ˆë‹¤. ë‚ ì”¨ê°€ ì ì  ì„ ì„ í•´ ì§€ê³  ìˆì–´ìš”. ê±´ê°• ì¡°ì‹¬í•˜ì„¸ìš”.\n\n ì™¼ìª½ ë©”ë‰´ì˜ [í™œìš© ì—…ë¬´ ì„ íƒ]ì„ í™•ì¸í•˜ê³  ê¶ê¸ˆí•œ ì‚¬í•­ì´ ìˆìœ¼ë©´ ì•„ë˜ì— ì…ë ¥í•´ ì£¼ì„¸ìš”.",
        "ìš´ë™í•˜ê¸° ì¢‹ì€ ë‚ ì„ ê¸°ëŒ€í•˜ë©° Jabisê°€ ì—¬ëŸ¬ë¶„ì„ ì‘ì›í•©ë‹ˆë‹¤.\n\n ì™¼ìª½ ë©”ë‰´ì˜ [í™œìš© ì—…ë¬´ ì„ íƒ]ì„ í™•ì¸í•˜ê³  ê¶ê¸ˆí•œ ì‚¬í•­ì´ ìˆìœ¼ë©´ ì•„ë˜ì— ì…ë ¥í•´ ì£¼ì„¸ìš”.",
        "Jabis ì…ë‹ˆë‹¤. ê±´ê°•ì„ ìœ„í•´ ë°°ë“œë¯¼í„´ì„ ê¶Œì¥í•©ë‹ˆë‹¤.\n\n ì™¼ìª½ ë©”ë‰´ì˜ [í™œìš© ì—…ë¬´ ì„ íƒ]ì„ í™•ì¸í•˜ê³  ê¶ê¸ˆí•œ ì‚¬í•­ì´ ìˆìœ¼ë©´ ì•„ë˜ì— ì…ë ¥í•´ ì£¼ì„¸ìš”.",
    ]

    random_number = random.randint(0, len(initial_messages) - 1)
    return initial_messages[random_number]


# í…ìŠ¤íŠ¸ ë¬¸ì„œ stream í•˜ê¸°
# -----------------------
def stream_data(greeting_message):
    for word in greeting_message.split(" "):
        yield word + " "
        time.sleep(0.05)


# ì‚¬ìš©ì í™”ë©´ ì‘ì„±í•˜ê¸°
# --------------------
log_writer(LOG_STATUS, "(jabis_chat.py) ì‚¬ìš©ì í™”ë©´ Start")


if "sbstate" not in st.session_state:
    st.session_state.sbstate = "collapsed"


with st.sidebar:
    clear_btn = st.button("ëŒ€í™” ì´ˆê¸°í™”")
    selected_prompt = st.selectbox(
        "í™œìš© ì—…ë¬´ ì„ íƒ ", ("ì€í–‰ì—…ë¬´ ì§ˆì˜", "ì¼ë°˜ ì§ˆì˜"), index=0
    )


# Header ì¶œë ¥
# ----------
head_col1, head_col2 = st.columns([0.9, 0.1], gap="large", vertical_alignment="center")
head_col1.subheader(
    "ğŸ’¬ :blue[JABIS(_Jb Ai Business Information System_)]",
    divider="rainbow",
    anchor=None,
    help=None,
)
# head_col1.header('ğŸ’¬ :blue[JABIS(_JBB Office ChatBot_)]', divider='rainbow', anchor=None, help=None)
head_col2.page_link("jabis.py", label="Home", icon="ğŸ ")


# ëŒ€í™”ë‚´ìš© ì €ì¥ ê³µê°„ ì •ì˜
# -----------------------
if "messages" not in st.session_state:
    # ëŒ€í™” ê¸°ë¡ì„ ì €ì¥í•˜ê¸° ìœ„í•œ ìš©ë„ë¡œ ìƒì„±í•œë‹¤.
    st.session_state["messages"] = []

    # ì²«ì¸ì‚¬ ì²˜ë¦¬
    # ------------
    greeting_message = greeting()
    st.chat_message("assistant").write_stream(stream_data(greeting_message))


# ì´ì „ ëŒ€í™”ë‚´ìš© ì €ì¥ ê³µê°„ ì •ì˜
# -----------------------
# if "chat_store" not in st.session_state:
#     st.session_state["chat_store"] = {}
# store = st.session_state["chat_store"]


# í™”ë©´ì— ì¶œë ¥ë˜ëŠ” ëŒ€í™” ë‚´ìš© ì‚­ì œ
# -----------------------
if clear_btn:
    st.session_state["messages"] = []


# ì´ì „ì˜ ëŒ€í™”ë‚´ìš©ì„ í™”ë©´ì— ì¶œë ¥
# -----------------------
for chat_message in st.session_state["messages"]:
    st.chat_message(chat_message.role).write(chat_message.content)


# ì‚¬ìš©ìì˜ ì…ë ¥
# ----------
user_input = st.chat_input("ê¶ê¸ˆí•œ ë‚´ìš©ì„ ë¬¼ì–´ë³´ì„¸ìš”!")


# ì‚¬ìš©ì ì§ˆë¬¸ ì…ë ¥ ë° ì²˜ë¦¬
# -------------------
if user_input:
    # ì‚¬ìš©ìì˜ ì…ë ¥
    st.chat_message("user").write(user_input)

    # chain, document_source, document_page = create_chain(selected_prompt, user_input)
    log_writer(LOG_STATUS, "(jabis_chat.py) create_chain() Start")
    chain, search_results = create_chain(selected_prompt, user_input)

    # ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ
    log_writer(LOG_STATUS, "(jabis_chat.py) chain.stream() Start")
    response = chain.stream(
        {"question": user_input}, config={"configurable": {"session_id": session_id}}
    )
    log_writer(LOG_STATUS, "(jabis_chat.py) chain.stream() End")

    with st.chat_message("assistant"):
        # ë¹ˆ ê³µê°„(ì»¨í…Œì´ë„ˆ)ì„ ë§Œë“¤ì–´ì„œ, ì—¬ê¸°ì— í† í°ì„ ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥ì„ í•œë‹¤.
        container = st.empty()

        ai_answer = ""
        for token in response:
            ai_answer += token
            container.markdown(ai_answer)
            # container.write(ai_answer)

        if selected_prompt == "ì€í–‰ì—…ë¬´ ì§ˆì˜":
            # í–¥í›„ ë¬¸ì„œ ë° í˜ì´ì§€ ì¤‘ë³µì‹œ ì¤‘ë³µ ì œê±°
            # ------------------------------------
            # test_list = set([(search_result[0].metadata['source'], search_result[0].metadata['page']) for search_result in search_results])
            # st.write(test_list)

            linked_docs = ""
            for search_result in search_results:
                if search_result[1] < 0.8:
                    # base_url = 'http://jabis.jbbank.co.kr:8080/jabis_pdf_view'
                    base_url = "http:/localhost:8501/jabis_pdf_view"
                    # base_url = 'http:/jabis.jbbank.co.kr/jabis_pdf_view'
                    # base_url = 'http:/jabis.jbbank.co.kr/jabis_pdf_view'
                    base_url = "jabis_pdf_view"

                    params = {
                        "source": search_result[0].metadata["source"],
                        "title": search_result[0].metadata["title"],
                        # 'title': search_result[0].metadata['source'],
                        "page": search_result[0].metadata["page"] + 1,
                    }
                    url_with_params = base_url + "?" + urlencode(params)

                    linked_docs += f"ğŸ‘‰ [{params['title']}]({url_with_params}) [pages]: {params['page']} [{round(search_result[1],3)}]\n\n"

            ai_answer = ai_answer + "\n\n ğŸ“– ê´€ë ¨ ë¬¸ì„œ ë³´ê¸°\n\n" + linked_docs
            container.markdown(ai_answer, unsafe_allow_html=True)

    # í™”ë©´ì— ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ì¶œë ¥í•˜ê¸° ìœ„í•´ ì €ì¥
    # --------------------------------
    add_message("user", user_input)
    add_message("assistant", ai_answer)

    # ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ëª¨ë¸ì— ì „ë‹¬í•˜ê¸° ìœ„í•´ ì €ì¥
    # --------------------------------
    # st.write(ai_answer) # TODO: delete
    # memory.save_context({"human": user_input}, {"assistant": ai_answer})
    # st.write(memory.load_memory_variables({}))
    log_writer(LOG_STATUS, "(jabis_chat.py) Jabis Chat Program End")
